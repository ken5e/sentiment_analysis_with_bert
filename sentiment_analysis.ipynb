{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eef0a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import transformers\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d46bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3ce5d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0 class                                               text\n",
      "0              0   Neg   In 1992 we bought a new Taurus and we really ...\n",
      "1              1   Neg   The last business trip  I drove to San Franci...\n",
      "2              2   Neg   My husband and I purchased a 1990 Ford F250 a...\n",
      "3              3   Neg   I feel I have a thorough opinion of this truc...\n",
      "4              4   Neg   AS a mother of 3  all of whom are still in ca...\n",
      "...          ...   ...                                                ...\n",
      "1219        1377   Pos   In June we bought the Sony Limited Edition Fo...\n",
      "1220        1378   Pos   After 140 000 miles  we decided to replace my...\n",
      "1221        1379   Pos   The Ford Focus is a great little record setti...\n",
      "1222        1380   Pos   I needed a new car because my hyundai excel 9...\n",
      "1223        1381   Pos   The 2000 Ford Focus SE 4 door sedan has a spa...\n",
      "\n",
      "[1224 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ca0e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "class      \n",
       "Neg     612\n",
       "Pos     612"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1a\n",
    "df.groupby(\"class\").count().drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec060a",
   "metadata": {},
   "source": [
    "There are 612 positive text messages and 612 negative text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7885aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0 class                                               text  \\\n",
      "0              0   Neg   In 1992 we bought a new Taurus and we really ...   \n",
      "1              1   Neg   The last business trip  I drove to San Franci...   \n",
      "2              2   Neg   My husband and I purchased a 1990 Ford F250 a...   \n",
      "3              3   Neg   I feel I have a thorough opinion of this truc...   \n",
      "4              4   Neg   AS a mother of 3  all of whom are still in ca...   \n",
      "...          ...   ...                                                ...   \n",
      "1219        1377   Pos   In June we bought the Sony Limited Edition Fo...   \n",
      "1220        1378   Pos   After 140 000 miles  we decided to replace my...   \n",
      "1221        1379   Pos   The Ford Focus is a great little record setti...   \n",
      "1222        1380   Pos   I needed a new car because my hyundai excel 9...   \n",
      "1223        1381   Pos   The 2000 Ford Focus SE 4 door sedan has a spa...   \n",
      "\n",
      "      text_length  \n",
      "0             831  \n",
      "1            1566  \n",
      "2            2063  \n",
      "3            3392  \n",
      "4            1599  \n",
      "...           ...  \n",
      "1219         1861  \n",
      "1220         2680  \n",
      "1221         1961  \n",
      "1222         1422  \n",
      "1223         2652  \n",
      "\n",
      "[1224 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1b\n",
    "df[\"text_length\"] = df[\"text\"].str.len()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dec1932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Neg</td>\n",
       "      <td>In 1992 we bought a new Taurus and we really ...</td>\n",
       "      <td>831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Neg</td>\n",
       "      <td>The last business trip  I drove to San Franci...</td>\n",
       "      <td>1566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Neg</td>\n",
       "      <td>My husband and I purchased a 1990 Ford F250 a...</td>\n",
       "      <td>2063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Neg</td>\n",
       "      <td>I feel I have a thorough opinion of this truc...</td>\n",
       "      <td>3392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Neg</td>\n",
       "      <td>AS a mother of 3  all of whom are still in ca...</td>\n",
       "      <td>1599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>1377</td>\n",
       "      <td>Pos</td>\n",
       "      <td>In June we bought the Sony Limited Edition Fo...</td>\n",
       "      <td>1861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>1378</td>\n",
       "      <td>Pos</td>\n",
       "      <td>After 140 000 miles  we decided to replace my...</td>\n",
       "      <td>2680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>1379</td>\n",
       "      <td>Pos</td>\n",
       "      <td>The Ford Focus is a great little record setti...</td>\n",
       "      <td>1961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>1380</td>\n",
       "      <td>Pos</td>\n",
       "      <td>I needed a new car because my hyundai excel 9...</td>\n",
       "      <td>1422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>1381</td>\n",
       "      <td>Pos</td>\n",
       "      <td>The 2000 Ford Focus SE 4 door sedan has a spa...</td>\n",
       "      <td>2652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 class                                               text  \\\n",
       "0              0   Neg   In 1992 we bought a new Taurus and we really ...   \n",
       "1              1   Neg   The last business trip  I drove to San Franci...   \n",
       "2              2   Neg   My husband and I purchased a 1990 Ford F250 a...   \n",
       "3              3   Neg   I feel I have a thorough opinion of this truc...   \n",
       "4              4   Neg   AS a mother of 3  all of whom are still in ca...   \n",
       "...          ...   ...                                                ...   \n",
       "1219        1377   Pos   In June we bought the Sony Limited Edition Fo...   \n",
       "1220        1378   Pos   After 140 000 miles  we decided to replace my...   \n",
       "1221        1379   Pos   The Ford Focus is a great little record setti...   \n",
       "1222        1380   Pos   I needed a new car because my hyundai excel 9...   \n",
       "1223        1381   Pos   The 2000 Ford Focus SE 4 door sedan has a spa...   \n",
       "\n",
       "      text_length  label  \n",
       "0             831      0  \n",
       "1            1566      0  \n",
       "2            2063      0  \n",
       "3            3392      0  \n",
       "4            1599      0  \n",
       "...           ...    ...  \n",
       "1219         1861      1  \n",
       "1220         2680      1  \n",
       "1221         1961      1  \n",
       "1222         1422      1  \n",
       "1223         2652      1  \n",
       "\n",
       "[1224 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2a\n",
    "# We try to drop data with missing text, but there is no such data.\n",
    "df = df.drop(df[df.text_length == 0].index)\n",
    "df[\"label\"] = df[\"class\"].apply(lambda x: 1 if x == \"Pos\" else 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23ddb9",
   "metadata": {},
   "source": [
    "# 2b \n",
    "We need to tokenzier the sentence before we do anything\n",
    "We can convert the text into a vector before training, but for simplicity, we train the Bert model together with the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d51adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the class of text dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, messages, targets, tokenizer, max_len):\n",
    "        self.messages = messages\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.messages[item])\n",
    "        target = int(self.targets[item])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': pt.tensor(target, dtype=pt.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a17c4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloader\n",
    "df_train, df_test = train_test_split(df, test_size=0.1)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5)\n",
    "df_train.text\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 4\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    padding='max_length',\n",
    "    model_max_length=512, \n",
    "    Trucation=True\n",
    ")\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = TextDataset(\n",
    "        messages=df.text.to_numpy(),\n",
    "        targets=df.label.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, max_len, batch_size)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1900c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q3 Define the  model, we use a bert-base-uncased model with \n",
    "class SentimentClassifier(pt.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = pt.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        return self.out(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18e1cc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "device = \"cuda\"\n",
    "\n",
    "model = SentimentClassifier(2)\n",
    "optimizer = pt.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = pt.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17051b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "  model = model.to(device)\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = pt.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += pt.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00589bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  model = model.to(device)\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with pt.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = pt.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += pt.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa9a0d56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Ken\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.5903748342416425 accuracy 0.7020890099909173\n",
      "Val   loss 0.38238796312361956 accuracy 0.8360655737704918\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.43887626877445995 accuracy 0.8664850136239781\n",
      "Val   loss 0.47961423319065943 accuracy 0.8688524590163935\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.22072551590753248 accuracy 0.9445958219800181\n",
      "Val   loss 0.6622770473768469 accuracy 0.8852459016393444\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.12208261865062936 accuracy 0.9754768392370572\n",
      "Val   loss 1.0789962846247363 accuracy 0.8524590163934427\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.07370962042203746 accuracy 0.9881925522252497\n",
      "Val   loss 1.2165836756612407 accuracy 0.8688524590163935\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.05447819325248323 accuracy 0.9909173478655767\n",
      "Val   loss 1.2898190013020212 accuracy 0.8524590163934427\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.03593133532938183 accuracy 0.9927338782924614\n",
      "Val   loss 1.3976047946107428 accuracy 0.8524590163934427\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.01466883462431935 accuracy 0.9954586739327883\n",
      "Val   loss 1.4408891835091708 accuracy 0.8524590163934427\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.008326369238773992 accuracy 0.997275204359673\n",
      "Val   loss 1.4807509258571372 accuracy 0.8688524590163935\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.009734748670609037 accuracy 0.997275204359673\n",
      "Val   loss 1.4858921827317317 accuracy 0.8524590163934427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc.cpu())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.cpu())\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        pt.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89ce90f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a8a4936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\Ken\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7903225806451613"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of the model in the testing set\n",
    "model = SentimentClassifier(2)\n",
    "model.load_state_dict(pt.load('best_model_state.bin'))\n",
    "model = model.to(device)\n",
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c23c174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    "\n",
    "    with pt.no_grad():\n",
    "        for d in data_loader:\n",
    "            texts = d[\"text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = pt.max(outputs, dim=1)\n",
    "\n",
    "            probs = nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "            texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(probs)\n",
    "            real_values.extend(targets)\n",
    "\n",
    "    predictions = pt.stack(predictions).cpu()\n",
    "    prediction_probs = pt.stack(prediction_probs).cpu()\n",
    "    real_values = pt.stack(real_values).cpu()\n",
    "    return texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4037fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e657d6c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.82      0.81        33\n",
      "    positive       0.79      0.76      0.77        29\n",
      "\n",
      "    accuracy                           0.79        62\n",
      "   macro avg       0.79      0.79      0.79        62\n",
      "weighted avg       0.79      0.79      0.79        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q4 we use precision, recall, etc... to evaluate the classification model\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "# precise = TP / (TP + FP)\n",
    "# recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a30516d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  6],\n",
       "       [ 7, 22]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8fcce72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my ford focus had some of the same problems as volvinator  quot this car sucks quot  namely the leak and the brake problem  i ve had it in for the brakes twice and now they tell me that because of the high metal content in the brake pads the squeaking is unavoidable  i ve called customer no care and they have been unhelpful even with the best of intentions  i too am facing the 12 000 mile deadline  and one simply incompetent customer service agent even suggested that i just not drive the car so that i don t break the 12k barrier and all my repairs would be covered when the brake pad is developed  when will that be  next week  next month  or next year  she couldn t answer that question  at the mailbox this evening i met another focus owner who has the cruise control problem where if you put it into cruise the car gets a mind of its own and starts speeding away  she also is having the brake problem  and since she has power locks she has one door that refuses to lock when she presses the button from the driver s seat  the only outstanding aspect of this car is the gas mileage  especially while gas is so expensive  an update on 11 16 2000 i brought my car in to fix the leak into the passenger side through the glove box one month ago  they took my car for a total of 11 days without offering me a rental  and when i asked  they said they don t do anything like that  i found out subsequent to this that ford service is REQUIRED to provide a substitute car upon request  although i haven t tried it since then  so i can t vouch that they ll actually do it  well  it is now november 16  and today  for the first time in a long time  we got a nice steady downpour all day  i did not use my car at all today  and tonight  as i am a mere 8 hours away from my 10 day thanksgiving vacation 1 000 miles away  i notice a large  steadily growing puddle on the passenger side floor  the leak is back  and with my trip tomorrow  i have no options but to drive it to new jersey and see if northern ford dealers are more accommodating than southerners  again  just one more reason that i am disappointed with my decision to purchase this car  stay tuned for more details when i try to bring it in again br br  \n",
      "\n",
      "prediction: negative with probability 0.9979749321937561\n",
      "True sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Draw one sample and see the prediction result\n",
    "idx = 2\n",
    "\n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "prob, pred = pt.max(y_pred_probs[idx], dim=0)\n",
    "pred = \"negative\" if pred == 0 else 1\n",
    "\n",
    "print(review_text)\n",
    "print()\n",
    "print(f\"prediction: {pred} with probability {prob}\")\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936434f",
   "metadata": {},
   "source": [
    "5a. I choose this model because Bert model is a powerful pretrained model for multiple natural language processing tasks, whether is upstream or downstream. Basically speaking, for any natural language processing tasks, one can try out Bert model to see the performance. I only add a linear layer after the Bert model to make my model output a valid probablity.\n",
    "\n",
    "5b. Accuracy, precision, recall , F1-score\n",
    "\n",
    "5c. The bert model can handle sentence with length at most 512, everything longer than that will be truncated. That will make our model blind to long sentence. My model also use a cross entropy loss to train, which may be affected by random noise. \n",
    "\n",
    "5d. One can try to break the longer sentence into multiple chunks and use some bagging ideas to handle a longer sentence. Or one can also try a more powerful model like GPT-3 to model to sentiment. For the loss function, one can also try to use hinge loss to make the model more robust to noise. Actually, the code contains boilerplate which can be simplified by using another framework called pytorch-lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc0b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
